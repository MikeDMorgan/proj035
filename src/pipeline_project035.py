##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Mike Morgan
:Release: $Id$
:Date: |today|
:Tags: Python

Overview
========

Analysis pipeline for project035 - Does exposure to a high fat diet lead to
effects on the germline epigenome?

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_project035.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGATPipelines.PipelineTracks as PipelineTracks
import PipelineProject035 as P35
import glob
import itertools


# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


GENESETS = PipelineTracks.Tracks(PipelineTracks.Sample).loadFromDirectory(
    glob.glob("*.gtf.gz"),
    "(\S+).gtf.gz")

TRACKS = PipelineTracks.Tracks(PipelineTracks.Sample3)
TRACKS = TRACKS.loadFromDirectory(glob.glob("*.bam"),
                                  "(\S+).bam")
REPLICATE = PipelineTracks.Aggregate(TRACKS, labels=("replicate"))
CONDITION = PipelineTracks.Aggregate(TRACKS, labels=("condition", "tissue"))

# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions


def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
@follows(mkdir("feature_counts.dir"))
@files([(("%s.bam" % x.asFile(), "%s.gtf.gz" % y.asFile()),
         ("feature_counts.dir/%s_vs_%s.tsv.gz" % (x.asFile(),
                                                  y.asFile())))
        for x, y in itertools.product(TRACKS, GENESETS)])
def buildFeatureCounts(infiles, outfile):
    '''counts reads falling into "features", which by default are genes.

    A read overlaps if at least one bp overlaps.

    Pairs and strandedness can be used to resolve reads falling into
    more than one feature. Reads that cannot be resolved to a single
    feature are ignored.

    '''

    infile, annotations = infiles

    # featureCounts cannot handle gzipped in or out files
    outfile = P.snip(outfile, ".gz")
    annotations_tmp = P.getTempFilename()

    # -p -B specifies count fragments rather than reads, and both
    # reads must map to the feature
    if PARAMS['featurecounts_paired'] == "1":
        paired = "-p -B"
    else:
        paired = ""

    job_options = "-pe dedicated %i" % PARAMS['featurecounts_threads']

    statement = '''
    zcat %(annotations)s > %(annotations_tmp)s;
    checkpoint;
    featureCounts %(featurecounts_options)s
    -T %(featurecounts_threads)s
    -s %(featurecounts_strand)s
    -g gene_id
    -b
    -a %(annotations_tmp)s
    -o %(outfile)s
    %(infile)s
    > %(outfile)s.log;
    checkpoint;
    gzip %(outfile)s;
    checkpoint;
    rm %(annotations_tmp)s '''

    P.run()


@follows(buildFeatureCounts)
@collate(buildFeatureCounts,
         regex("feature_counts.dir/(.+)-(.+)-(.+)_vs_(.+).tsv.gz"),
         r"feature_counts.dir/\1-\4-feature_counts.tsv.gz")
def aggregateFeatureCounts(infiles, outfile):
    ''' build a matrix of counts with genes and tracks dimensions.
    '''

    # Use column 7 as counts. This is a possible source of bugs, the
    # column position has changed before.

    infiles = " ".join(infiles)
    statement = '''
    python %(scriptsdir)s/combine_tables.py
    --columns=1
    --take=7
    --use-file-prefix
    --regex-filename='(.+)_vs.+.tsv.gz'
    --log=%(outfile)s.log
    %(infiles)s
    | sed 's/Geneid/gene_id/'
    | sed 's/\-/\./g'
    | tee %(outfile)s.table.tsv
    | gzip > %(outfile)s '''

    P.run()


@follows(aggregateFeatureCounts)
@transform(aggregateFeatureCounts,
           suffix(".tsv.gz"),
           ".load")
def loadFeatureCounts(infile, outfile):
    P.load(infile, outfile, "--add-index=gene_id")

# ---------------------------------------------------


@follows(loadFeatureCounts,
         mkdir("deseq2.dir"),
         mkdir("images.dir"))
@transform(aggregateFeatureCounts,
           regex("feature_counts.dir/(.+)-(.+)-feature_counts.tsv.gz"),
           add_inputs("design.tsv"),
           r"deseq2.dir/\1-\2-diff_genes.tsv")
def counts2DiffGenes(infiles, outfile):
    '''
    Perform differential expression analysis using DESeq2
    Output MA-plots, PCA plots (1st 6 PCs), heatmap of sample
    clustering, results table
    '''

    reference = infiles[0].split("/")[-1].split("-")[1]
    counts_file = infiles[0]
    design_file = infiles[1]

    P35.deseqAnalysis(counts_table=counts_file,
                      design=design_file,
                      reference=reference,
                      outfile=outfile,
                      submit=True)


@follows(counts2DiffGenes)
@transform(counts2DiffGenes,
           suffix(".tsv"),
           ".load")
def loadDiffGenes(infile, outfile):
    P.load(infile, outfile)

# ---------------------------------------------------


@follows(loadDiffGenes,
         mkdir("spike_in.dir/"))
@transform(aggregateFeatureCounts,
           regex("feature_counts.dir/(.+)-(.+)-feature_counts.tsv.gz"),
           add_inputs("design.tsv"),
           r"spike_in.dir/\1-\2-spike_in.tsv.gz")
def spikeInCounts(infiles, outfile):
    '''
    Perform spike-in across a specific range of fold changes or absolute
    count differences.  Counts table generated from original input counts
    data.
    '''

    counts_file = infiles[0]
    design_file = infiles[1]

    statement = '''
    zcat %(counts_file)s |
    python %(scriptsdir)s/counts2counts.py --design-tsv-file=%(design_file)s
            --method="spike" --spike-type="row" --spike-change-bin-max=3.0
            --spike-change-bin-width=0.1  --spike-change-bin-min=0.1
            --spike-initial-bin-width=1
            --spike-initial-bin-min=1 --spike-initial-bin-max=200000
            --spike-minimum=1 --spike-maximum=1000000
            --random-seed=%(random_seed)i
            --spike-iterations=%(spike_iterations)i  -v 5
            --log=%(outfile)s.log
            | gzip > %(outfile)s
    '''

    P.run()


@follows(spikeInCounts)
@transform(spikeInCounts,
           suffix(".tsv.gz"),
           ".load")
def loadSpikeIn(infile, outfile):
    P.load(infile, outfile)


@follows(loadSpikeIn)
@transform(spikeInCounts,
           regex("spike_in.dir/(.+)-(.+)-spike_in.tsv.gz"),
           add_inputs("design.tsv"),
           r"deseq2.dir/\1-\2-spike_in-diff_genes.tsv")
def DESeqSpikeInAnalysis(infiles, outfile):
    '''
    Perform differential expression testing using in silico spike-in
    counts table.
    '''

    reference = infiles[0].split("/")[-1].split("-")[1] + "_spikeIn"
    counts_file = infiles[0]
    design_file = infiles[1]

    P35.deseqAnalysis(counts_table=counts_file,
                      design=design_file,
                      reference=reference,
                      outfile=outfile,
                      submit=True)


@follows(DESeqSpikeInAnalysis)
@transform(DESeqSpikeInAnalysis,
           suffix(".tsv"),
           ".load")
def loadSpikeInDESeq(infile, outfile):
    P.load(infile, outfile)

# Generic pipeline tasks


@follows(loadDiffGenes,
         loadSpikeInDESeq)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
